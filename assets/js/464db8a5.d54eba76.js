"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[1817],{28453:(e,n,t)=>{t.d(n,{R:()=>a,x:()=>o});var r=t(96540);const s={},i=r.createContext(s);function a(e){const n=r.useContext(i);return r.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:a(e.components),r.createElement(i.Provider,{value:n},e.children)}},79292:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>c,contentTitle:()=>o,default:()=>u,frontMatter:()=>a,metadata:()=>r,toc:()=>l});const r=JSON.parse('{"id":"GenAI/AIApplications/rag_guide","title":"Retrieval-Augmented Generation (RAG): A Comprehensive Guide","description":"In-depth guide to Retrieval-Augmented Generation (RAG) for engineers and data scientists\u2014covering core concepts, technical architecture, implementation, advanced techniques, and real-world applications.","source":"@site/docs/GenAI/AIApplications/rag_guide.md","sourceDirName":"GenAI/AIApplications","slug":"/GenAI/rag-guide","permalink":"/blogs/docs/GenAI/rag-guide","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/GenAI/AIApplications/rag_guide.md","tags":[{"inline":true,"label":"rag","permalink":"/blogs/docs/tags/rag"},{"inline":true,"label":"retrieval-augmented-generation","permalink":"/blogs/docs/tags/retrieval-augmented-generation"},{"inline":true,"label":"llm","permalink":"/blogs/docs/tags/llm"},{"inline":true,"label":"ai","permalink":"/blogs/docs/tags/ai"},{"inline":true,"label":"genai","permalink":"/blogs/docs/tags/genai"},{"inline":true,"label":"architecture","permalink":"/blogs/docs/tags/architecture"},{"inline":true,"label":"implementation","permalink":"/blogs/docs/tags/implementation"},{"inline":true,"label":"best-practices","permalink":"/blogs/docs/tags/best-practices"}],"version":"current","frontMatter":{"title":"Retrieval-Augmented Generation (RAG): A Comprehensive Guide","description":"In-depth guide to Retrieval-Augmented Generation (RAG) for engineers and data scientists\u2014covering core concepts, technical architecture, implementation, advanced techniques, and real-world applications.","slug":"/GenAI/rag-guide","authors":["wiseagent"],"tags":["rag","retrieval-augmented-generation","llm","ai","genai","architecture","implementation","best-practices"],"sidebar_label":"RAG Comprehensive Guide"},"sidebar":"WISeAgentSidebar","previous":{"title":"MCP Complete Guide","permalink":"/blogs/docs/GenAI/AIApplications/mcp-guide"},"next":{"title":"AI Studio Getting Started","permalink":"/blogs/docs/GenAI/gemini/aistudio-getting-started"}}');var s=t(74848),i=t(28453);const a={title:"Retrieval-Augmented Generation (RAG): A Comprehensive Guide",description:"In-depth guide to Retrieval-Augmented Generation (RAG) for engineers and data scientists\u2014covering core concepts, technical architecture, implementation, advanced techniques, and real-world applications.",slug:"/GenAI/rag-guide",authors:["wiseagent"],tags:["rag","retrieval-augmented-generation","llm","ai","genai","architecture","implementation","best-practices"],sidebar_label:"RAG Comprehensive Guide"},o="Retrieval-Augmented Generation (RAG): A Comprehensive Guide",c={},l=[{value:"Table of Contents",id:"table-of-contents",level:2},{value:"1. Introduction: Why RAG Matters",id:"1-introduction-why-rag-matters",level:2},{value:"Key Benefits",id:"key-benefits",level:3},{value:"2. Core Concepts: How RAG Works",id:"2-core-concepts-how-rag-works",level:2},{value:"The Open-Book Exam Analogy",id:"the-open-book-exam-analogy",level:3},{value:"The RAG Pipeline",id:"the-rag-pipeline",level:3},{value:"3. Technical Architecture",id:"3-technical-architecture",level:2},{value:"Core Components",id:"core-components",level:3},{value:"3.1 The Retriever: Expert Librarian",id:"31-the-retriever-expert-librarian",level:4},{value:"3.2 The Generator: Eloquent Synthesizer",id:"32-the-generator-eloquent-synthesizer",level:4},{value:"3.3 Vector Embeddings Deep Dive",id:"33-vector-embeddings-deep-dive",level:3},{value:"4. Implementation Guide",id:"4-implementation-guide",level:2},{value:"4.1 Basic RAG Implementation",id:"41-basic-rag-implementation",level:3},{value:"4.2 Advanced Chunking Strategies",id:"42-advanced-chunking-strategies",level:3},{value:"4.3 Multi-Modal RAG",id:"43-multi-modal-rag",level:3},{value:"5. Real-World Applications",id:"5-real-world-applications",level:2},{value:"5.1 Customer Support Automation",id:"51-customer-support-automation",level:3},{value:"5.2 Legal Document Analysis",id:"52-legal-document-analysis",level:3},{value:"5.3 Scientific Research Assistant",id:"53-scientific-research-assistant",level:3},{value:"6. Advanced Techniques",id:"6-advanced-techniques",level:2},{value:"6.1 Hybrid Search",id:"61-hybrid-search",level:3},{value:"6.2 Re-ranking and Query Expansion",id:"62-re-ranking-and-query-expansion",level:3},{value:"6.3 Contextual Compression",id:"63-contextual-compression",level:3},{value:"7. Production Considerations",id:"7-production-considerations",level:2},{value:"7.1 Scalability Architecture",id:"71-scalability-architecture",level:3},{value:"7.2 Security and Privacy",id:"72-security-and-privacy",level:3},{value:"7.3 Cost Optimization",id:"73-cost-optimization",level:3},{value:"8. Common Pitfalls and Solutions",id:"8-common-pitfalls-and-solutions",level:2},{value:"8.1 Data Quality Issues",id:"81-data-quality-issues",level:3},{value:"8.2 Retrieval Irrelevance",id:"82-retrieval-irrelevance",level:3},{value:"8.3 Context Window Limitations",id:"83-context-window-limitations",level:3},{value:"9. Evaluation and Optimization",id:"9-evaluation-and-optimization",level:2},{value:"9.1 Evaluation Metrics",id:"91-evaluation-metrics",level:3}];function d(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",h4:"h4",header:"header",hr:"hr",li:"li",mermaid:"mermaid",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,i.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.header,{children:(0,s.jsx)(n.h1,{id:"retrieval-augmented-generation-rag-a-comprehensive-guide",children:"Retrieval-Augmented Generation (RAG): A Comprehensive Guide"})}),"\n",(0,s.jsx)(n.h2,{id:"table-of-contents",children:"Table of Contents"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"#1-introduction-why-rag-matters",children:"Introduction: Why RAG Matters"})}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"#2-core-concepts-how-rag-works",children:"Core Concepts: How RAG Works"})}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"#3-technical-architecture",children:"Technical Architecture"})}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"#4-implementation-guide",children:"Implementation Guide"})}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"#5-real-world-applications",children:"Real-World Applications"})}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"#6-advanced-techniques",children:"Advanced Techniques"})}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"#7-production-considerations",children:"Production Considerations"})}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"#8-common-pitfalls-and-solutions",children:"Common Pitfalls and Solutions"})}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"#9-evaluation-and-optimization",children:"Evaluation and Optimization"})}),"\n"]}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"1-introduction-why-rag-matters",children:"1. Introduction: Why RAG Matters"}),"\n",(0,s.jsx)(n.p,{children:"Large Language Models (LLMs) have revolutionized how we interact with information, but they face fundamental limitations:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Knowledge cut-off"}),": Their information is frozen at training time"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Hallucination"}),": They can generate plausible but factually incorrect information"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Domain specificity"}),": They lack access to proprietary or specialized knowledge bases"]}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Retrieval-Augmented Generation (RAG)"})," addresses these challenges by connecting LLMs to external knowledge sources, enabling them to access current, factual, and domain-specific information before generating responses."]}),"\n",(0,s.jsx)(n.h3,{id:"key-benefits",children:"Key Benefits"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Accuracy"}),": Eliminates hallucinations by grounding responses in verified sources"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Currency"}),": Provides access to real-time and up-to-date information"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Customization"}),": Enables LLMs to work with proprietary or specialized data"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Transparency"}),": Users can trace answers back to source documents"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Cost-effectiveness"}),": Updates knowledge without expensive model retraining"]}),"\n"]}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"2-core-concepts-how-rag-works",children:"2. Core Concepts: How RAG Works"}),"\n",(0,s.jsx)(n.h3,{id:"the-open-book-exam-analogy",children:"The Open-Book Exam Analogy"}),"\n",(0,s.jsx)(n.p,{children:"Think of RAG as transforming an LLM from a student taking a closed-book exam (relying only on memorized training data) to one taking an open-book exam (consulting specific, approved materials before answering)."}),"\n",(0,s.jsx)(n.h3,{id:"the-rag-pipeline",children:"The RAG Pipeline"}),"\n",(0,s.jsx)(n.p,{children:"The RAG process follows three core steps:"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Retrieve"}),": Find relevant information from a knowledge base"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Augment"}),": Combine the retrieved context with the original query"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Generate"}),": Use the LLM to synthesize a response based on the provided context"]}),"\n"]}),"\n",(0,s.jsx)(n.mermaid,{value:"graph LR\n    A[User Query] --\x3e B[Retrieve Relevant Documents]\n    B --\x3e C[Augment Query with Context]\n    C --\x3e D[Generate Response]\n    D --\x3e E[Final Answer]"}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"3-technical-architecture",children:"3. Technical Architecture"}),"\n",(0,s.jsx)(n.h3,{id:"core-components",children:"Core Components"}),"\n",(0,s.jsx)(n.h4,{id:"31-the-retriever-expert-librarian",children:"3.1 The Retriever: Expert Librarian"}),"\n",(0,s.jsx)(n.p,{children:"The Retriever's role is to find the most relevant information from your knowledge base."}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Process:"})}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Document Processing"}),": Source documents are chunked into manageable segments"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Embedding Generation"}),": Text chunks are converted to vector embeddings using specialized models"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Vector Storage"}),": Embeddings are stored in vector databases optimized for similarity search"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Query Processing"}),": User queries are embedded using the same model"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Similarity Search"}),": The system finds chunks with embeddings most similar to the query"]}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Key Technologies:"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Embedding Models"}),": OpenAI's text-embedding-3-large, sentence-transformers, Cohere Embed"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Vector Databases"}),": Pinecone, Weaviate, Chroma, Qdrant, Milvus"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Similarity Metrics"}),": Cosine similarity, Euclidean distance, dot product"]}),"\n"]}),"\n",(0,s.jsx)(n.h4,{id:"32-the-generator-eloquent-synthesizer",children:"3.2 The Generator: Eloquent Synthesizer"}),"\n",(0,s.jsx)(n.p,{children:"The Generator is an LLM that creates human-readable responses using both the original query and retrieved context."}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Popular Models:"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"OpenAI GPT-4 series"}),"\n",(0,s.jsx)(n.li,{children:"Anthropic Claude"}),"\n",(0,s.jsx)(n.li,{children:"Meta Llama 2/3"}),"\n",(0,s.jsx)(n.li,{children:"Google Gemini"}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"33-vector-embeddings-deep-dive",children:"3.3 Vector Embeddings Deep Dive"}),"\n",(0,s.jsx)(n.p,{children:"Vector embeddings are numerical representations that capture semantic meaning:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'# Example: Text to embedding conversion\ntext = "What causes climate change?"\nembedding = embedding_model.encode(text)\n# Result: [0.234, -0.567, 0.891, ...] (typically 768-4096 dimensions)\n'})}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Quality factors:"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Dimensionality"}),": Higher dimensions can capture more nuanced relationships"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Training data"}),": Models trained on domain-relevant data perform better"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Context window"}),": Longer context windows preserve more semantic information"]}),"\n"]}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"4-implementation-guide",children:"4. Implementation Guide"}),"\n",(0,s.jsx)(n.h3,{id:"41-basic-rag-implementation",children:"4.1 Basic RAG Implementation"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'from langchain.document_loaders import WebBaseLoader, PyPDFLoader\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom langchain.embeddings import OpenAIEmbeddings\nfrom langchain.vectorstores import Chroma\nfrom langchain.chat_models import ChatOpenAI\nfrom langchain.chains import RetrievalQA\n\n# Step 1: Document Loading and Processing\ndef setup_knowledge_base(sources):\n    documents = []\n    \n    # Load documents from various sources\n    for source in sources:\n        if source.endswith(\'.pdf\'):\n            loader = PyPDFLoader(source)\n        else:\n            loader = WebBaseLoader(source)\n        documents.extend(loader.load())\n    \n    # Split documents into chunks\n    text_splitter = RecursiveCharacterTextSplitter(\n        chunk_size=1000,\n        chunk_overlap=200,\n        separators=["\\n\\n", "\\n", ".", "!", "?", ",", " ", ""]\n    )\n    chunks = text_splitter.split_documents(documents)\n    \n    # Create vector store\n    embeddings = OpenAIEmbeddings(model="text-embedding-3-large")\n    vectorstore = Chroma.from_documents(\n        documents=chunks,\n        embedding=embeddings,\n        persist_directory="./vectorstore"\n    )\n    \n    return vectorstore\n\n# Step 2: RAG Chain Setup\ndef create_rag_chain(vectorstore):\n    llm = ChatOpenAI(\n        model="gpt-4",\n        temperature=0.1  # Lower temperature for factual responses\n    )\n    \n    retriever = vectorstore.as_retriever(\n        search_type="similarity",\n        search_kwargs={"k": 5}  # Retrieve top 5 relevant chunks\n    )\n    \n    # Create RAG chain with custom prompt\n    qa_chain = RetrievalQA.from_chain_type(\n        llm=llm,\n        chain_type="stuff",\n        retriever=retriever,\n        return_source_documents=True,\n        chain_type_kwargs={\n            "prompt": create_custom_prompt()\n        }\n    )\n    \n    return qa_chain\n\ndef create_custom_prompt():\n    from langchain.prompts import PromptTemplate\n    \n    template = """Use the following context to answer the question. \n    If you cannot find the answer in the context, say "I don\'t have enough information to answer this question."\n    \n    Context: {context}\n    \n    Question: {question}\n    \n    Answer: """\n    \n    return PromptTemplate(\n        template=template,\n        input_variables=["context", "question"]\n    )\n\n# Step 3: Usage\ndef query_rag_system(chain, question):\n    result = chain({"query": question})\n    \n    return {\n        "answer": result["result"],\n        "sources": [doc.metadata.get("source", "Unknown") \n                   for doc in result["source_documents"]]\n    }\n\n# Example usage\nif __name__ == "__main__":\n    sources = ["company_docs.pdf", "https://example.com/knowledge-base"]\n    vectorstore = setup_knowledge_base(sources)\n    rag_chain = create_rag_chain(vectorstore)\n    \n    response = query_rag_system(rag_chain, "What is our company\'s remote work policy?")\n    print(f"Answer: {response[\'answer\']}")\n    print(f"Sources: {response[\'sources\']}")\n'})}),"\n",(0,s.jsx)(n.h3,{id:"42-advanced-chunking-strategies",children:"4.2 Advanced Chunking Strategies"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'def semantic_chunking(documents, embedding_model, similarity_threshold=0.8):\n    """Implement semantic-based chunking instead of fixed-size chunks."""\n    chunks = []\n    current_chunk = []\n    \n    sentences = split_into_sentences(documents)\n    embeddings = embedding_model.encode(sentences)\n    \n    for i, sentence in enumerate(sentences):\n        if not current_chunk:\n            current_chunk.append(sentence)\n            continue\n            \n        # Calculate similarity with current chunk\n        chunk_embedding = np.mean([embeddings[j] for j in range(len(current_chunk))], axis=0)\n        similarity = cosine_similarity([embeddings[i]], [chunk_embedding])[0][0]\n        \n        if similarity > similarity_threshold:\n            current_chunk.append(sentence)\n        else:\n            # Start new chunk\n            chunks.append(\' \'.join(current_chunk))\n            current_chunk = [sentence]\n    \n    # Add final chunk\n    if current_chunk:\n        chunks.append(\' \'.join(current_chunk))\n    \n    return chunks\n'})}),"\n",(0,s.jsx)(n.h3,{id:"43-multi-modal-rag",children:"4.3 Multi-Modal RAG"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'from langchain.document_loaders import UnstructuredFileLoader\nfrom langchain.schema import Document\n\ndef process_multimodal_documents(file_path):\n    """Process documents containing text, images, and tables."""\n    \n    # Extract text, images, and tables\n    loader = UnstructuredFileLoader(file_path, mode="elements")\n    elements = loader.load()\n    \n    documents = []\n    for element in elements:\n        if element.metadata.get("category") == "Image":\n            # Process image with vision model\n            image_description = describe_image(element.metadata["image_path"])\n            doc = Document(\n                page_content=f"Image description: {image_description}",\n                metadata={"type": "image", "source": file_path}\n            )\n            documents.append(doc)\n        elif element.metadata.get("category") == "Table":\n            # Process table structure\n            table_content = structure_table(element.page_content)\n            doc = Document(\n                page_content=f"Table data: {table_content}",\n                metadata={"type": "table", "source": file_path}\n            )\n            documents.append(doc)\n        else:\n            # Regular text processing\n            documents.append(element)\n    \n    return documents\n'})}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"5-real-world-applications",children:"5. Real-World Applications"}),"\n",(0,s.jsx)(n.h3,{id:"51-customer-support-automation",children:"5.1 Customer Support Automation"}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Scenario"}),": E-commerce company automates product support using RAG."]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'class CustomerSupportRAG:\n    def __init__(self):\n        self.setup_knowledge_base([\n            "product_manuals/",\n            "faq_database.json",\n            "troubleshooting_guides/"\n        ])\n    \n    def handle_support_query(self, query, customer_data):\n        # Enhance query with customer context\n        enhanced_query = f"""\n        Customer Profile: {customer_data.get(\'tier\', \'standard\')} customer\n        Purchase History: {customer_data.get(\'recent_purchases\', [])}\n        \n        Support Question: {query}\n        """\n        \n        response = self.rag_chain(enhanced_query)\n        \n        # Add escalation logic\n        if self.requires_human_escalation(response):\n            return self.create_escalation_ticket(query, response)\n        \n        return response\n'})}),"\n",(0,s.jsx)(n.h3,{id:"52-legal-document-analysis",children:"5.2 Legal Document Analysis"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'class LegalRAG:\n    def __init__(self):\n        self.setup_specialized_embeddings()  # Legal domain-specific embeddings\n        self.load_legal_documents([\n            "case_law_database/",\n            "statutes/",\n            "regulations/"\n        ])\n    \n    def analyze_contract(self, contract_text, analysis_type="risk_assessment"):\n        relevant_cases = self.retrieve_similar_cases(contract_text)\n        applicable_laws = self.retrieve_applicable_statutes(contract_text)\n        \n        analysis_prompt = f"""\n        Analyze this contract for {analysis_type}.\n        \n        Contract: {contract_text}\n        \n        Relevant Case Law: {relevant_cases}\n        Applicable Statutes: {applicable_laws}\n        \n        Provide analysis with specific legal citations.\n        """\n        \n        return self.llm.generate(analysis_prompt)\n'})}),"\n",(0,s.jsx)(n.h3,{id:"53-scientific-research-assistant",children:"5.3 Scientific Research Assistant"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'class ResearchRAG:\n    def __init__(self):\n        self.setup_scientific_databases([\n            "arxiv_papers/",\n            "pubmed_abstracts/",\n            "patent_database/"\n        ])\n    \n    def literature_review(self, research_topic, date_range=None):\n        # Retrieve relevant papers with recency bias\n        papers = self.retrieve_papers(\n            topic=research_topic,\n            date_range=date_range,\n            citation_threshold=10  # Minimum citations\n        )\n        \n        # Generate comprehensive literature review\n        review = self.generate_review(papers, research_topic)\n        \n        return {\n            "summary": review,\n            "key_papers": papers[:10],\n            "research_gaps": self.identify_gaps(papers, research_topic),\n            "methodology_trends": self.analyze_methodologies(papers)\n        }\n'})}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"6-advanced-techniques",children:"6. Advanced Techniques"}),"\n",(0,s.jsx)(n.h3,{id:"61-hybrid-search",children:"6.1 Hybrid Search"}),"\n",(0,s.jsx)(n.p,{children:"Combines semantic search with traditional keyword search for improved retrieval:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"from rank_bm25 import BM25Okapi\n\nclass HybridRetriever:\n    def __init__(self, documents):\n        # Semantic search setup\n        self.vector_store = self.create_vector_store(documents)\n        \n        # Keyword search setup\n        tokenized_docs = [doc.split() for doc in documents]\n        self.bm25 = BM25Okapi(tokenized_docs)\n        self.documents = documents\n    \n    def retrieve(self, query, alpha=0.7):\n        # Semantic search\n        semantic_scores = self.vector_store.similarity_search_with_score(query, k=20)\n        \n        # Keyword search\n        keyword_scores = self.bm25.get_scores(query.split())\n        \n        # Combine scores\n        combined_scores = {}\n        for doc, score in semantic_scores:\n            combined_scores[doc.page_content] = alpha * score\n        \n        for i, score in enumerate(keyword_scores):\n            doc_content = self.documents[i]\n            if doc_content in combined_scores:\n                combined_scores[doc_content] += (1 - alpha) * score\n            else:\n                combined_scores[doc_content] = (1 - alpha) * score\n        \n        # Return top results\n        return sorted(combined_scores.items(), key=lambda x: x[1], reverse=True)[:5]\n"})}),"\n",(0,s.jsx)(n.h3,{id:"62-re-ranking-and-query-expansion",children:"6.2 Re-ranking and Query Expansion"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"from sentence_transformers import CrossEncoder\n\nclass AdvancedRAG:\n    def __init__(self):\n        self.base_retriever = self.setup_base_retriever()\n        self.reranker = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')\n        self.query_expander = self.setup_query_expansion_model()\n    \n    def enhanced_retrieve(self, query, top_k=5):\n        # Step 1: Query expansion\n        expanded_queries = self.query_expander.expand(query)\n        \n        # Step 2: Initial retrieval with expanded queries\n        all_candidates = []\n        for exp_query in expanded_queries:\n            candidates = self.base_retriever.get_relevant_documents(exp_query)\n            all_candidates.extend(candidates)\n        \n        # Step 3: De-duplication and re-ranking\n        unique_candidates = self.deduplicate(all_candidates)\n        \n        # Step 4: Re-rank using cross-encoder\n        query_doc_pairs = [(query, doc.page_content) for doc in unique_candidates]\n        rerank_scores = self.reranker.predict(query_doc_pairs)\n        \n        # Step 5: Return top-k re-ranked results\n        scored_docs = list(zip(unique_candidates, rerank_scores))\n        return sorted(scored_docs, key=lambda x: x[1], reverse=True)[:top_k]\n"})}),"\n",(0,s.jsx)(n.h3,{id:"63-contextual-compression",children:"6.3 Contextual Compression"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'from langchain.retrievers import ContextualCompressionRetriever\nfrom langchain.retrievers.document_compressors import LLMChainExtractor\n\ndef setup_compression_retriever(base_retriever, llm):\n    """Create a retriever that compresses retrieved documents."""\n    \n    compressor = LLMChainExtractor.from_llm(\n        llm=llm,\n        prompt_template="""\n        Extract only the information directly relevant to the question.\n        Remove any irrelevant context while preserving key facts and details.\n        \n        Question: {question}\n        \n        Document: {context}\n        \n        Relevant information:\n        """\n    )\n    \n    return ContextualCompressionRetriever(\n        base_compressor=compressor,\n        base_retriever=base_retriever\n    )\n'})}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"7-production-considerations",children:"7. Production Considerations"}),"\n",(0,s.jsx)(n.h3,{id:"71-scalability-architecture",children:"7.1 Scalability Architecture"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"import asyncio\nfrom concurrent.futures import ThreadPoolExecutor\nimport redis\n\nclass ProductionRAG:\n    def __init__(self):\n        # Redis for caching\n        self.redis_client = redis.Redis(host='localhost', port=6379, db=0)\n        \n        # Thread pool for parallel processing\n        self.executor = ThreadPoolExecutor(max_workers=10)\n        \n        # Load balancing for multiple vector stores\n        self.vector_stores = self.setup_distributed_vector_stores()\n    \n    async def query_with_caching(self, query):\n        # Check cache first\n        cache_key = f\"rag_query:{hash(query)}\"\n        cached_result = self.redis_client.get(cache_key)\n        \n        if cached_result:\n            return json.loads(cached_result)\n        \n        # Parallel retrieval from multiple stores\n        tasks = [\n            self.query_vector_store(store, query) \n            for store in self.vector_stores\n        ]\n        \n        results = await asyncio.gather(*tasks)\n        \n        # Merge and rank results\n        final_result = self.merge_results(results)\n        \n        # Cache result\n        self.redis_client.setex(\n            cache_key, \n            3600,  # 1 hour TTL\n            json.dumps(final_result)\n        )\n        \n        return final_result\n    \n    def setup_monitoring(self):\n        \"\"\"Setup monitoring for production RAG system.\"\"\"\n        import logging\n        from prometheus_client import Counter, Histogram\n        \n        # Metrics\n        self.query_counter = Counter('rag_queries_total', 'Total RAG queries')\n        self.response_time = Histogram('rag_response_time_seconds', 'RAG response time')\n        self.retrieval_accuracy = Histogram('rag_retrieval_accuracy', 'Retrieval accuracy')\n        \n        # Logging\n        logging.basicConfig(\n            level=logging.INFO,\n            format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n            handlers=[\n                logging.FileHandler('rag_system.log'),\n                logging.StreamHandler()\n            ]\n        )\n"})}),"\n",(0,s.jsx)(n.h3,{id:"72-security-and-privacy",children:"7.2 Security and Privacy"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'class SecureRAG:\n    def __init__(self):\n        self.user_permissions = self.load_user_permissions()\n        self.data_classifier = self.setup_data_classification()\n    \n    def secure_query(self, query, user_id):\n        # Input validation and sanitization\n        if not self.validate_query(query):\n            raise ValueError("Invalid query format")\n        \n        # Check user permissions\n        allowed_sources = self.get_user_sources(user_id)\n        \n        # Filter retrieval based on permissions\n        filtered_retriever = self.create_filtered_retriever(allowed_sources)\n        \n        # Perform retrieval with access controls\n        results = filtered_retriever.get_relevant_documents(query)\n        \n        # Post-process to remove sensitive information\n        sanitized_results = self.sanitize_results(results, user_id)\n        \n        # Audit logging\n        self.log_access(user_id, query, [r.metadata.get(\'source\') for r in results])\n        \n        return sanitized_results\n    \n    def detect_prompt_injection(self, query):\n        """Detect potential prompt injection attacks."""\n        injection_patterns = [\n            r"ignore previous instructions",\n            r"system prompt",\n            r"you are now",\n            r"forget everything"\n        ]\n        \n        for pattern in injection_patterns:\n            if re.search(pattern, query.lower()):\n                return True\n        return False\n'})}),"\n",(0,s.jsx)(n.h3,{id:"73-cost-optimization",children:"7.3 Cost Optimization"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'class CostOptimizedRAG:\n    def __init__(self):\n        self.embedding_cache = {}\n        self.cheap_llm = ChatOpenAI(model="gpt-3.5-turbo")  # Cheaper for initial filtering\n        self.expensive_llm = ChatOpenAI(model="gpt-4")      # More expensive for final generation\n    \n    def tiered_generation(self, query, context):\n        # Use cheaper model for initial assessment\n        initial_response = self.cheap_llm.predict(f"""\n        Assess if this query requires complex reasoning or can be answered simply.\n        Respond with \'SIMPLE\' or \'COMPLEX\'.\n        \n        Query: {query}\n        Context: {context[:500]}  # Truncated context for cheaper processing\n        """)\n        \n        if "SIMPLE" in initial_response:\n            return self.cheap_llm.predict(f"Context: {context}\\n\\nQuery: {query}")\n        else:\n            return self.expensive_llm.predict(f"Context: {context}\\n\\nQuery: {query}")\n    \n    def batch_embedding_generation(self, texts, batch_size=100):\n        """Generate embeddings in batches to reduce API costs."""\n        all_embeddings = []\n        \n        for i in range(0, len(texts), batch_size):\n            batch = texts[i:i + batch_size]\n            \n            # Check cache first\n            cached_embeddings = []\n            uncached_texts = []\n            \n            for text in batch:\n                text_hash = hashlib.sha256(text.encode()).hexdigest()\n                if text_hash in self.embedding_cache:\n                    cached_embeddings.append(self.embedding_cache[text_hash])\n                else:\n                    uncached_texts.append(text)\n            \n            # Generate embeddings only for uncached texts\n            if uncached_texts:\n                new_embeddings = self.embedding_model.encode(uncached_texts)\n                \n                # Cache new embeddings\n                for text, embedding in zip(uncached_texts, new_embeddings):\n                    text_hash = hashlib.sha256(text.encode()).hexdigest()\n                    self.embedding_cache[text_hash] = embedding\n                \n                all_embeddings.extend(new_embeddings)\n            \n            all_embeddings.extend(cached_embeddings)\n        \n        return all_embeddings\n'})}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"8-common-pitfalls-and-solutions",children:"8. Common Pitfalls and Solutions"}),"\n",(0,s.jsx)(n.h3,{id:"81-data-quality-issues",children:"8.1 Data Quality Issues"}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Problem"}),": Poor source data leads to incorrect or unhelpful responses."]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Solutions"}),":"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'def validate_document_quality(documents):\n    """Validate and clean documents before indexing."""\n    validated_docs = []\n    \n    for doc in documents:\n        # Check document length\n        if len(doc.page_content) < 50:\n            continue  # Skip very short documents\n        \n        # Check for meaningful content\n        if is_meaningful_content(doc.page_content):\n            # Clean and normalize\n            cleaned_doc = clean_document(doc)\n            validated_docs.append(cleaned_doc)\n    \n    return validated_docs\n\ndef is_meaningful_content(text):\n    """Check if document contains meaningful content."""\n    # Remove common non-meaningful patterns\n    patterns_to_remove = [\n        r"^Table of Contents",\n        r"^Page \\d+ of \\d+",\n        r"^Copyright \\d{4}",\n        r"^\\s*\\d+\\s*$"  # Pages with only numbers\n    ]\n    \n    for pattern in patterns_to_remove:\n        if re.match(pattern, text.strip()):\n            return False\n    \n    # Check for minimum word count and variety\n    words = text.split()\n    unique_words = set(words)\n    \n    return len(words) >= 20 and len(unique_words) / len(words) > 0.3\n'})}),"\n",(0,s.jsx)(n.h3,{id:"82-retrieval-irrelevance",children:"8.2 Retrieval Irrelevance"}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Problem"}),": Retrieved documents are not relevant to the query."]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Solutions"}),":"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'class ImprovedRetrieval:\n    def __init__(self):\n        self.relevance_threshold = 0.7\n        self.max_results = 10\n    \n    def filtered_retrieval(self, query, vector_store):\n        # Get more candidates than needed\n        candidates = vector_store.similarity_search_with_score(\n            query, \n            k=self.max_results * 2\n        )\n        \n        # Filter by relevance threshold\n        relevant_docs = [\n            (doc, score) for doc, score in candidates \n            if score >= self.relevance_threshold\n        ]\n        \n        # If too few results, lower threshold gradually\n        if len(relevant_docs) < 3:\n            threshold = self.relevance_threshold - 0.1\n            relevant_docs = [\n                (doc, score) for doc, score in candidates \n                if score >= threshold\n            ]\n        \n        return [doc for doc, _ in relevant_docs[:self.max_results]]\n    \n    def query_expansion(self, original_query):\n        """Expand query using synonyms and related terms."""\n        expansion_prompt = f"""\n        Generate 2-3 alternative phrasings of this query that maintain the same meaning:\n        \n        Original: {original_query}\n        \n        Alternatives:\n        1.\n        2.\n        3.\n        """\n        \n        expanded = self.llm.predict(expansion_prompt)\n        return [original_query] + self.parse_alternatives(expanded)\n'})}),"\n",(0,s.jsx)(n.h3,{id:"83-context-window-limitations",children:"8.3 Context Window Limitations"}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Problem"}),": Too much retrieved context overwhelms the LLM's context window."]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Solutions"}),":"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'def manage_context_window(retrieved_docs, max_tokens=8000):\n    """Intelligently manage context to fit within token limits."""\n    import tiktoken\n    \n    encoding = tiktoken.encoding_for_model("gpt-4")\n    current_tokens = 0\n    selected_docs = []\n    \n    # Sort documents by relevance score\n    sorted_docs = sorted(retrieved_docs, key=lambda x: x[1], reverse=True)\n    \n    for doc, score in sorted_docs:\n        doc_tokens = len(encoding.encode(doc.page_content))\n        \n        if current_tokens + doc_tokens <= max_tokens:\n            selected_docs.append(doc)\n            current_tokens += doc_tokens\n        else:\n            # Try to fit a truncated version\n            remaining_tokens = max_tokens - current_tokens\n            if remaining_tokens > 100:  # Minimum meaningful chunk\n                truncated_content = truncate_to_tokens(\n                    doc.page_content, \n                    remaining_tokens, \n                    encoding\n                )\n                truncated_doc = Document(\n                    page_content=truncated_content,\n                    metadata=doc.metadata\n                )\n                selected_docs.append(truncated_doc)\n            break\n    \n    return selected_docs\n\ndef truncate_to_tokens(text, max_tokens, encoding):\n    """Truncate text to fit within token limit while preserving meaning."""\n    tokens = encoding.encode(text)\n    \n    if len(tokens) <= max_tokens:\n        return text\n    \n    # Truncate at sentence boundaries when possible\n    sentences = text.split(\'. \')\n    truncated_text = ""\n    current_tokens = 0\n    \n    for sentence in sentences:\n        sentence_tokens = len(encoding.encode(sentence + \'. \'))\n        if current_tokens + sentence_tokens <= max_tokens:\n            truncated_text += sentence + \'. \'\n            current_tokens += sentence_tokens\n        else:\n            break\n    \n    return truncated_text.strip()\n'})}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"9-evaluation-and-optimization",children:"9. Evaluation and Optimization"}),"\n",(0,s.jsx)(n.h3,{id:"91-evaluation-metrics",children:"9.1 Evaluation Metrics"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'class RAGEvaluator:\n    def __init__(self):\n        self.ground_truth_dataset = self.load_ground_truth()\n    \n    def evaluate_retrieval_quality(self, queries, retrieved_docs):\n        """Evaluate retrieval component quality."""\n        metrics = {\n            \'precision_at_k\': self.calculate_precision_at_k(queries, retrieved_docs),\n            \'recall_at_k\': self.calculate_recall_at_k(queries, retrieved_docs),\n            \'mrr\': self.calculate_mrr(queries, retrieved_docs),  # Mean Reciprocal Rank\n            \'ndcg\': self.calculate_ndcg(queries, retrieved_docs)  # Normalized Discounted Cumulative Gain\n        }\n        return metrics\n    \n    def calculate_precision_at_k(self, queries, retrieved_docs, k=5):\n        """Calculate precision@k for retrieval."""\n        precisions = []\n        \n        for query, docs in zip(queries, retrieved_docs):\n            relevant_docs = self.get_relevant_docs(query)\n            retrieved_ids = [doc.metadata.get(\'id\') for doc in docs[:k]]\n            relevant_ids = [doc.metadata.get(\'id\') for doc in relevant_docs]\n            \n            relevant_retrieved = set(retrieved_ids) & set(relevant_ids)\n            precision = len(relevant_retrieved) / k if k > 0 else 0\n            precisions.append(precision)\n        \n        return sum(precisions) / len(precisions)\n    \n    def evaluate_answer_quality(self, questions, generated_answers, reference_answers):\n        """Evaluate generation component quality."""\n        from sentence_transformers import SentenceTransformer\n        import numpy as np\n        \n        # Semantic similarity evaluation\n        model = SentenceTransformer(\'all-MiniLM-L6-v2\')\n        \n        semantic_scores = []\n        for gen_answer, ref_answer in zip(generated_answers, reference_answers):\n            gen_embedding = model.encode([gen_answer])\n            ref_embedding = model.encode([ref_answer])\n            similarity = np.dot(gen_embedding, ref_embedding.T)[0][0]\n            semantic_scores.append(similarity)\n        \n        # Factual accuracy evaluation (using LLM-as-judge)\n        accuracy_scores = []\n        for question, gen_answer, ref_answer in zip(questions, generated_answers, reference_answers):\n            accuracy_score = self.evaluate_factual_accuracy(question, gen_answer, ref_answer)\n            accuracy_scores.append(accuracy_score)\n        \n        return {\n            \'semantic_similarity\': np.mean(semantic_scores),\n            \'factual_accuracy\': np.mean(accuracy_scores),\n            \'average_length\': np.mean([len(answer.split()) for answer in generated_answers])\n        }\n    \n    def evaluate_factual_accuracy(self, question, generated_answer, reference_answer):\n        """Use\n'})})]})}function u(e={}){const{wrapper:n}={...(0,i.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(d,{...e})}):d(e)}}}]);